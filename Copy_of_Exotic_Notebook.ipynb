{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Exotic Notebook.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kaleetock/astro/blob/main/Copy_of_Exotic_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xYSqIvTTQSU"
      },
      "source": [
        "# Exoplanet Transit Interpretation code (EXOTIC)\n",
        "\n",
        "### <font color='red'>Click File --> \"Save a copy in Drive\"</font> to make your own copy of this notebook. \n",
        "\n",
        "Then run the cells of this notebook in sequence by clicking the \"play\" button at the left of each code cell.  [Here](https://drive.google.com/file/d/10zlQRgT8iV3dSe0FVW7tiL-V86ewai_1/view?usp=sharing) is a screencast that walks you through the process with the sample data (view at high-resolution).  EXOTIC will use a virtual machine in the cloud to run the code, but it will use images in your Google drive account as input.  Therefore, you need to give it permissions to access your Google drive account filesystem.  (Because of this, you may want to use a throwaway Google drive account as temporary storage for your image files.)\n",
        "\n",
        "Most of the code that is used for the steps in this Colaboratory has been hidden, but if you want to see it, double-click on the corresponding code cell.  (To re-hide the code, right-click on the left sidebar of the cell and select Form-->\"Hide Code\".)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQ8r43woH4wY",
        "cellView": "form"
      },
      "source": [
        "#@title <font color='red'>Click \"run\"</font> (at left) to install EXOTIC and give it permission to run on your Google drive files.\n",
        "%%capture\n",
        "!pip install exotic --upgrade\n",
        "import bokeh.io\n",
        "# from exotic.api.plotting import plot_image\n",
        "from exotic.exotic import NASAExoplanetArchive, get_wcs, find_target\n",
        "from astropy.time import Time\n",
        "from barycorrpy import utc_tdb\n",
        "import numpy as np\n",
        "from io import BytesIO\n",
        "from astropy.io import fits\n",
        "from scipy.ndimage import label\n",
        "from bokeh.plotting import figure, output_file, show\n",
        "from bokeh.palettes import Viridis256\n",
        "from bokeh.models import ColorBar, LinearColorMapper, LogColorMapper, LogTicker\n",
        "from bokeh.models import BoxZoomTool,WheelZoomTool,ResetTool,HoverTool,PanTool,FreehandDrawTool\n",
        "from bokeh.io import output_notebook\n",
        "from pprint import pprint\n",
        "from IPython.display import Image\n",
        "from ipywidgets import widgets, HBox\n",
        "from skimage.transform import rescale, resize, downscale_local_mean\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "!pip install dynesty\n",
        "import dynesty\n",
        "from dynesty import plotting\n",
        "from dynesty.utils import resample_equal\n",
        "from scipy.stats import gaussian_kde\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import subprocess\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYU6gaN1DvDG"
      },
      "source": [
        "Wait until the cell above is finished running before proceeding.  You will know that it is done running because the Colaboratory logo at the top of the tab turns from gray to yellow.  (In general, a gray Colaboratory logo means \"running\", yellow means \"ready for the next step\", and red means \"something is wrong\".)  Note that there is an incrementing timer plus \"headline\" command strings scrolling across the status bar at the bottom of the browser window; these are replaced with a total run time and date stamp on completion of the cell. \n",
        "\n",
        "Once it is done, look up your target in the NASA Exoplanet Archive.  In order to fit a lightcurve with appropriate priors, EXOTIC needs to know when the planet is expected to transit and how much starlight the planet is expected to block when it does.  (If you know what an inits file is and you already have one, then you do not *need* to run this target lookup cell, but it is always a good check.)\n",
        "\n",
        "If you are running the sample data, then your target is Hat-p-32 b.  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7lvRl5wKHBZ",
        "cellView": "form"
      },
      "source": [
        "#@title <font color='red'>Click \"run\"</font> to look up your target in the NASA Exoplanet Archive.\n",
        "\n",
        "##############################################################\n",
        "\n",
        "def fix_planetary_params (p_param_dict):\n",
        "  for param in p_param_dict.keys():\n",
        "    if param == \"Target Star RA\" or param == \"Target Star Dec\" or param == \"Planet Name\" or param == \"Host Star Name\":\n",
        "      continue\n",
        "    val = p_param_dict[param]\n",
        "    if val == 0.0 or np.isnan(float(val)):\n",
        "      if param == \"Orbital Eccentricity (0 if null)\":\n",
        "        continue\n",
        "      if param == \"Ratio of Planet to Stellar Radius (Rp/Rs)\":\n",
        "        p_param_dict[param] = 0.151\n",
        "      if param == \"Ratio of Planet to Stellar Radius (Rp/Rs) Uncertainty\":\n",
        "        p_param_dict[param] = 0.151\n",
        "        if p_param_dict[\"Host Star Name\"] == \"Qatar-6\":\n",
        "          p_param_dict[param] = 0.01\n",
        "      print(f\"\\nIn the planetary parameters from the NASA Exoplanet Archive, \\n\\\"{param}\\\" is listed as {val}.\\n\\n**** This might make EXOTIC crash. ****\\n\\nIf the parameter is *not* changed below, please edit it\\nin the inits file before running EXOTIC.\\n\")\n",
        "  p_param_string = json.dumps(p_param_dict)\n",
        "\n",
        "  planetary_params = \"\\\"planetary_parameters\\\": {\\n\"\n",
        "  num_done, num_total = 0, len(p_param_dict.keys())\n",
        "  for key, value in p_param_dict.items():\n",
        "    num_done += 1\n",
        "    if key == \"Target Star RA\" or key == \"Target Star Dec\" or key == \"Planet Name\" or key == \"Host Star Name\":\n",
        "      planetary_params = planetary_params + str(f\"    \\\"{key}\\\": \\\"{value}\\\",\\n\")\n",
        "    else:\n",
        "      if num_done < num_total:\n",
        "        planetary_params = planetary_params + str(f\"    \\\"{key}\\\": {value},\\n\")\n",
        "      else:\n",
        "        planetary_params = planetary_params + str(f\"    \\\"{key}\\\": {value}\\n\")\n",
        "  planetary_params = planetary_params + \"}\"\n",
        "\n",
        "  return(planetary_params)\n",
        "\n",
        "##############################################################\n",
        "\n",
        "target=input(\"Please enter the name of your exoplanet target: \")\n",
        "targ = NASAExoplanetArchive(planet=target)\n",
        "target = targ.planet_info()[0]\n",
        "planetary_params = \"\"\n",
        "if not targ.resolve_name():\n",
        "  print(\"Sorry, can't find your target in the Exoplanet Archive.  Unfortunately, this\")\n",
        "  print(\"isn't going to work until I can find it.  Please re-run this cell, trying\")\n",
        "  print(\"different formats for your target name, until the target is located.\")\n",
        "  print(\"Looking it up in the NASA Exoplanet Archive at https://exoplanetarchive.ipac.caltech.edu/\")\n",
        "  print(\"might help you know where to put the spaces and hyphens and such.\")\n",
        "else:\n",
        "  p_param_string = targ.planet_info(fancy=True)\n",
        "  planetary_params = \"\\\"planetary_parameters\\\": \"+p_param_string\n",
        "  p_param_dict = json.loads(p_param_string)\n",
        "  planetary_params = fix_planetary_params(p_param_dict)\n",
        "  print(f\"\\nNASA Exoplanet Archive planetary parameters for {target}:\\n\")\n",
        "  print(planetary_params)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5B58xAojJsAY"
      },
      "source": [
        "### Specifying the path to your images:\n",
        "* If you are a new user running the Hat-p-32b sample data, just press enter. This will download the sample data to your Google drive account, ask you to identify the target and pick some comparison stars.  The target star is at the center of [this starchart](https://app.aavso.org/vsp/chart/?star=HAT-P-32&scale=F&orientation=visual&type=chart&fov=18.5&maglimit=18.5&resolution=150&north=down&east=left), labeled with an empty circle.  AAVSO recomends using the numbered stars as comparison stars.  Once you find the stars in your image, EXOTIC will fit a lightcurve and save the output files in your Google drive account. \n",
        "* If you are reducing a single transit, enter the path to the google drive folder that holds your images. On your computer in browser\n",
        "it will look like, for example: My Drive > exotransits > Wasp-43. When asked for the folder, you would enter \"exotransits/Wasp-43.\"  Use the folder icon on the left menu bar to locate the folder, right-click on it, and select \"Copy path\".  If you have them, calibration images should be in folders named \"dark\", \"flat\", and \"bias\" *within* the folder containing your images.  The first image of the series will be displayed so that you can identify your target and pick some comparison stars (see [here](https://app.aavso.org/vsp/) for AAVSO recommended comps).  You will also be asked to specify any needed parameters that EXOTIC cannot find from your image headers.\n",
        "* If you know what an initialization file is and you already have one, put it into the folder with your images and enter the folder name as specified above.  If there is *exactly one* file with a name ending in .json in the folder with your images, this will automatically be used to initialize EXOTIC and reduce your images.\n",
        "* If you have two or more initialization files and you want to reduce multiple transits, enter the path to a folder that only contains your inits files.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npI5MbPvJnYA",
        "cellView": "form"
      },
      "source": [
        "#@title <font color='red'>Click \"run\"</font> to start the data reduction.\n",
        "# If the user presses enter to run the sample data, download sample data if needed and\n",
        "# put it into a sample-data directory at the top level of the user's Gdrive.  Count\n",
        "# the .fits files (images) and .json files (inits files) in the directory entered \n",
        "# by the user (or in the sample-data directory if the user pressed enter).  If \n",
        "# there are at least 20 .fits files, assume this is a directory of images and display\n",
        "# the first one in the series.  If there is exactly one inits file in the directory, \n",
        "# show the specified target and comp coords so that the user can check these against\n",
        "# the displayed image.  Otherwise, prompt for target / comp coords and make an inits \n",
        "# file based  on those (save this new inits file in the folder with the output files \n",
        "# so that the student can consult it later).  Finally, run EXOTIC with the newly-made \n",
        "# or pre-existing inits file, plus any other inits files in the directory.\n",
        "\n",
        "#########################################################\n",
        "\n",
        "def display_image(filename):\n",
        "    print(f\"{filename}\")\n",
        "    hdu = fits.open(filename)\n",
        "\n",
        "# replace 0's with extension      # Stuff to put in if the hdul ever becomes a problem.    \n",
        "#    extension = 0\n",
        "#    image_header = hdul[extension].header\n",
        "#    while image_header[\"NAXIS\"] == 0:\n",
        "#      extension += 1\n",
        "#      image_header = hdul[extension].header\n",
        "\n",
        "    dheader = dict(hdu[0].header)\n",
        "  \n",
        "    data = hdu[0].data\n",
        "    megapixel_factor = (data.shape[0])*(data.shape[1])/1000000.0\n",
        "    if megapixel_factor > 5:\n",
        "      print(f\"Downsampling image because it has {megapixel_factor} megapixels.\")\n",
        "      image_downscaled = downscale_local_mean(data, (2, 2)).astype(int)\n",
        "      data = image_downscaled\n",
        "    \n",
        "    max_y = len(data)\n",
        "    max_x = len(data[0])\n",
        "    p_height = 500\n",
        "    p_width = int((p_height/max_y) * max_x)\n",
        "\n",
        "    # quick hot pixel/ cosmic ray mask\n",
        "    # mask, cdata = detect_cosmics(\n",
        "    #     data, psfmodel='gauss',\n",
        "    #     psffwhm=4, psfsize=2*round(4)+1, # just a guess\n",
        "    #     sepmed=False, sigclip = 4.25,\n",
        "    #     niter=3, objlim=10, cleantype='idw', verbose=False\n",
        "    # )\n",
        "\n",
        "    # create a figure with text on mouse hover\n",
        "    fig = figure(tooltips=[(\"x\", \"$x\"), (\"y\", \"$y\"), (\"value\", \"@image\")], plot_width=p_width, plot_height=p_height,\n",
        "        tools=[PanTool(),BoxZoomTool(),WheelZoomTool(),ResetTool(),HoverTool()])\n",
        "    fig.x_range.range_padding = fig.y_range.range_padding = 0\n",
        "\n",
        "    r = fig.multi_line('x', 'y', source={'x':[],'y':[]},color='white',line_width=3)\n",
        "    fig.add_tools(FreehandDrawTool(renderers=[r]))\n",
        "\n",
        "    # set up a colobar + data range\n",
        "    color_mapper = LogColorMapper(palette=\"Cividis256\", low=np.percentile(data, 55), high=np.percentile(data, 99))\n",
        "\n",
        "    # must give a vector of image data for image parameter\n",
        "    fig.image(\n",
        "        image=[data],\n",
        "          x=0, y=0, dw=hdu[0].data.shape[1], dh=hdu[0].data.shape[0],\n",
        "          level=\"image\", color_mapper=color_mapper\n",
        "    )\n",
        "    fig.grid.grid_line_width = 0.5\n",
        "\n",
        "    color_bar = ColorBar(color_mapper=color_mapper, ticker=LogTicker(),\n",
        "                         label_standoff=12, border_line_color=None, location=(0,0))\n",
        "\n",
        "    fig.add_layout(color_bar, 'right')\n",
        "\n",
        "    show(fig)\n",
        "\n",
        "#########################################################\n",
        "\n",
        "def floats_to_ints(l):\n",
        "  while (True):\n",
        "#    print (l)\n",
        "    m = re.search(r\"^(.*?)(\\d+\\.\\d+)(.*?)$\", l)\n",
        "    if m:\n",
        "      start, fl, end = m.group(1), float(m.group(2)), m.group(3)\n",
        "      l = start+str(\"%.0f\" % fl)+end\n",
        "    else:\n",
        "      return(l)\n",
        "  \n",
        "#########################################################\n",
        "\n",
        "# Find a field in the image fits header or prompt the user to enter the corresponding\n",
        "# value.\n",
        "\n",
        "def check_dir(p):\n",
        "  p = p.replace(\"\\\\\", \"/\")\n",
        "\n",
        "  if not(os.path.isdir(p)):\n",
        "    print(f\"Problem: the directory {p} doesn't seem to exist\")\n",
        "    print(\"on your Gdrive filesystem.\")\n",
        "    return(\"\")\n",
        "  return(p)\n",
        "\n",
        "#########################################################\n",
        "\n",
        "def add_sign(var):\n",
        "  str_var = str(var)\n",
        "  m=re.search(r\"^[\\+\\-]\", str_var)\n",
        "  if m:\n",
        "    return(str_var)\n",
        "  if float(var) >= 0:\n",
        "    return(str(\"+%.6f\" % float(var)))\n",
        "  else:\n",
        "    return(str(\"-%.6f\" % float(var)))\n",
        "\n",
        "#########################################################\n",
        "\n",
        "def get_val(hdr, ks):\n",
        "  for key in ks:\n",
        "    if key in hdr.keys():\n",
        "      return hdr[key]\n",
        "    if key.lower() in hdr.keys():\n",
        "      return hdr[key.lower()]\n",
        "    new_key = key[0]+key[1:len(key)].lower()  # first letter capitalized\n",
        "    if new_key in hdr.keys():\n",
        "      return hdr[new_key]\n",
        "  return(\"\")\n",
        "\n",
        "#########################################################\n",
        "\n",
        "def process_lat_long(val, key):\n",
        "  m = re.search(r\"\\'?([+-]?\\d+)[\\s\\:](\\d+)[\\s\\:](\\d+\\.?\\d*)\", val)\n",
        "  if m:\n",
        "    deg, min, sec = float(m.group(1)), float(m.group(2)), float(m.group(3))\n",
        "    if deg < 0:\n",
        "      v = deg - (((60*min) + sec)/3600)\n",
        "    else:\n",
        "      v = deg + (((60*min) + sec)/3600)\n",
        "    return(add_sign(v))\n",
        "  m = re.search(\"^\\'?([+-]?\\d+\\.\\d+)\", val)\n",
        "  if m:\n",
        "    v = float(m.group(1))\n",
        "    return(add_sign(v))\n",
        "  else:\n",
        "    print(f\"Cannot match value {val}, which is meant to be {key}.\")\n",
        "\n",
        "#########################################################\n",
        "\n",
        "# Convert a MicroObservatory timestamp (which is in local time) to UTC.\n",
        "\n",
        "def convert_Mobs_to_utc(datestamp, latitude, longitude, height):\n",
        "\n",
        "#  print(datestamp)\n",
        "  t = Time(datestamp[0:21], format='isot', scale='utc')\n",
        "  t -= 0.33\n",
        "\n",
        "  return(str(t)[0:10])\n",
        "\n",
        "#########################################################\n",
        "\n",
        "def find (hdr, ks, obs):\n",
        "  # Special stuff for MObs and Boyce-Astro Observatories\n",
        "  boyce = {\"FILTER\": \"ip\", \"LATITUDE\": \"+32.6135\", \"LONGITUD\": \"-116.3334\", \"HEIGHT\": 1405 }\n",
        "  mobs = {\"FILTER\": \"V\", \"LATITUDE\": \"+37.04\", \"LONGITUD\": \"-110.73\", \"HEIGHT\": 2606 }\n",
        "\n",
        "  if \"OBSERVAT\" in hdr.keys() and hdr[\"OBSERVAT\"] == 'Whipple Observatory':\n",
        "    obs = \"MObs\"\n",
        "\n",
        "#  if \"USERID\" in hdr.keys() and hdr[\"USERID\"] == 'PatBoyce':\n",
        "#    obs = \"Boyce\"\n",
        "\n",
        "  if obs == \"Boyce\":\n",
        "    boyce_val = get_val(boyce, ks)\n",
        "    if (boyce_val != \"\"):\n",
        "      return(boyce_val)\n",
        "  if obs == \"MObs\":\n",
        "    mobs_val = get_val(mobs, ks)\n",
        "    if (mobs_val != \"\"):\n",
        "      return(mobs_val)\n",
        "\n",
        "  val = get_val(hdr, ks)\n",
        "\n",
        "  if ks[0] == \"LATITUDE\" and val != \"\":         # Because EXOTIC needs these with signs shown.\n",
        "    return(process_lat_long(str(val), \"latitude\"))\n",
        "  if ks[0] == \"LONGITUD\" and val != \"\":\n",
        "    return(process_lat_long(str(val), \"longitude\"))\n",
        "\n",
        "  if (val != \"\"):\n",
        "    return(val)\n",
        "\n",
        "  print(f\"\\nI cannot find a field with any of these names in your image header: \\n{ks}.\")\n",
        "  print(\"Please enter the value (not the name of the header field, the actual value) that should\")\n",
        "  print(\"be used for the value associated with this field.\\n\")\n",
        "  if ks[0] == \"HEIGHT\":\n",
        "    print(\"The units of elevation are meters.\")\n",
        "  value = input(\"\")\n",
        "  return(value)\n",
        "\n",
        "###############################################\n",
        "\n",
        "def look_for_calibration(image_dir):\n",
        "  darks_dir, flats_dir, biases_dir = \"null\", \"null\", \"null\"\n",
        "\n",
        "  m = re.search(r\"(.*?)(\\d\\d\\d\\d\\-\\d\\d\\-\\d\\d)\\/images\", image_dir)  # This handles the way I set up the MObs image paths for my seminar teams.\n",
        "  if m:\n",
        "    prefix, date = m.group(1), m.group(2)\n",
        "    darks = prefix+date+\"/darks\"\n",
        "    if os.path.isdir(darks):\n",
        "      darks_dir = str(\"\\\"\"+darks+\"\\\"\")\n",
        "      \n",
        "  d_names = [\"dark\", \"darks\", \"DARK\", \"DARKS\", \"Dark\", \"Darks\"]  # Possible names for calibration image directories.\n",
        "  f_names = [\"flat\", \"flats\", \"FLAT\", \"FLATS\", \"Flat\", \"Flats\"]\n",
        "  b_names = [\"bias\", \"biases\", \"BIAS\", \"BIASES\", \"Bias\", \"Biases\"]\n",
        "\n",
        "  for d in d_names:\n",
        "    if os.path.isdir(os.path.join(image_dir, d)):\n",
        "      darks_dir = str(\"\\\"\"+os.path.join(image_dir, d)+\"\\\"\")\n",
        "      break\n",
        "\n",
        "  for f in f_names:\n",
        "    if os.path.isdir(os.path.join(image_dir, f)):\n",
        "      flats_dir = str(\"\\\"\"+os.path.join(image_dir, f)+\"\\\"\")\n",
        "      break\n",
        "\n",
        "  for b in b_names:\n",
        "    if os.path.isdir(os.path.join(image_dir, b)):\n",
        "      biases_dir = str(\"\\\"\"+os.path.join(image_dir, b)+\"\\\"\")\n",
        "      break\n",
        "\n",
        "  return(darks_dir, flats_dir, biases_dir)\n",
        "\n",
        "###############################################\n",
        "\n",
        "# Writes a new inits file into the directory with the output plots.  This prompts\n",
        "# for needed information that it cannot find in the fits header of the first image.\n",
        "\n",
        "def make_inits_file(planetary_params, image_dir, output_dir, first_image, targ_coords, comp_coords, obs, aavso_obs_code, sample_data):\n",
        "  inits_file_path = output_dir+\"inits.json\"\n",
        "  hdul = fits.open(first_image)\n",
        "  hdr = dict(hdul[0].header)\n",
        "\n",
        "  min, max = \"null\", \"null\"\n",
        "  filter = find(hdr, ['FILTER', 'FILT'], obs)\n",
        "  if filter == \"w\":\n",
        "    filter = \"PanSTARRS-w\"\n",
        "    min = \"404\"\n",
        "    max = \"846\"\n",
        "  if filter == \"Clear\":\n",
        "    filter = \"V\"\n",
        "  if filter == \"ip\":\n",
        "    min = \"690\"\n",
        "    max = \"819\"\n",
        "  if filter == \"EXO\":\n",
        "    filter = \"CBB\"\n",
        "  if re.search(r\"Green\", filter, re.IGNORECASE):\n",
        "    filter = \"SG\"\n",
        "    \n",
        "  date_obs = find(hdr,[\"DATE\", \"DATE_OBS\", \"DATE-OBS\"], obs)\n",
        "  date_obs = date_obs.replace(\"/\", \"_\")\n",
        "  longitude = find(hdr,['LONGITUD', 'LONG', 'LONGITUDE', 'SITELONG'],obs)\n",
        "  latitude = find(hdr,['LATITUDE', 'LAT', 'SITELAT'],obs)\n",
        "  height = int(find(hdr, ['HEIGHT', 'ELEVATION', 'ELE', 'EL', 'OBSGEO-H', 'ALT-OBS', 'SITEELEV'], obs))\n",
        "  obs_notes = \"N/A\"\n",
        "  sec_obs_code = \"N/A\"\n",
        "\n",
        "  mobs_data = False\n",
        "  # For MObs, the date is local rather than UTC, so convert.\n",
        "  if \"OBSERVAT\" in hdr.keys() and hdr[\"OBSERVAT\"] == 'Whipple Observatory':\n",
        "    date_obs = convert_Mobs_to_utc(date_obs, latitude, longitude, height)\n",
        "    weather = hdr[\"WEATHER\"] \n",
        "    temps = float(hdr[\"TELTEMP\"]) - float(hdr[\"CAMTEMP\"])\n",
        "    obs_notes = str(\"First image seeing %s (0: poor, 100: excellent), Teltemp - Camtemp %.1f.  These observations were conducted with MicroObservatory, a robotic telescope network managed by the Harvard-Smithsonian Center for Astrophysics on behalf of NASA's Universe of Learning. This work is supported by NASA under award number NNX16AC65A to the Space Telescope Science Institute.\" % (weather, temps))\n",
        "    sec_obs_code = \"MOBS\"  \n",
        "    mobs_data = True\n",
        "  \n",
        "  if aavso_obs_code == \"\":\n",
        "      aavso_obs_code = \"N/A\"\n",
        "\n",
        "  obs_date = date_obs[0:10]\n",
        "  (darks_dir, flats_dir, biases_dir) = look_for_calibration(image_dir)\n",
        "\n",
        "  with open(inits_file_path, 'w') as inits_file:\n",
        "    inits_file.write(\"\"\"\n",
        "{\n",
        "  %s,\n",
        "    \"user_info\": {\n",
        "            \"Directory with FITS files\": \"%s\",\n",
        "            \"Directory to Save Plots\": \"%s\",\n",
        "            \"Directory of Flats\": %s,\n",
        "            \"Directory of Darks\": %s,\n",
        "            \"Directory of Biases\": %s,\n",
        "\n",
        "            \"AAVSO Observer Code (N/A if none)\": \"%s\",\n",
        "            \"Secondary Observer Codes (N/A if none)\": \"%s\",\n",
        "\n",
        "            \"Observation date\": \"%s\",\n",
        "            \"Obs. Latitude\": \"%s\",\n",
        "            \"Obs. Longitude\": \"%s\",\n",
        "            \"Obs. Elevation (meters)\": %d,\n",
        "            \"Camera Type (CCD or DSLR)\": \"CCD\",\n",
        "            \"Pixel Binning\": \"1x1\",\n",
        "            \"Filter Name (aavso.org/filters)\": \"%s\",\n",
        "            \"Observing Notes\": \"%s\",\n",
        "\n",
        "            \"Plate Solution? (y/n)\": \"y\",\n",
        "            \"Align Images? (y/n)\": \"y\",\n",
        "\n",
        "            \"Target Star X & Y Pixel\": %s,\n",
        "            \"Comparison Star(s) X & Y Pixel\": %s\n",
        "    },    \n",
        "    \"optional_info\": {\n",
        "            \"Pixel Scale (Ex: 5.21 arcsecs/pixel)\": null,\n",
        "            \"Filter Minimum Wavelength (nm)\": %s,\n",
        "            \"Filter Maximum Wavelength (nm)\": %s\n",
        "    }\n",
        "}\n",
        "\"\"\" % (planetary_params, image_dir, output_dir, flats_dir, darks_dir, biases_dir, \n",
        "       aavso_obs_code, sec_obs_code, obs_date, latitude, longitude, height, filter, \n",
        "       obs_notes, targ_coords, comp_coords, min, max))\n",
        "\n",
        "  print(\"\\nWithin your folder of images, there is now a new folder called EXOTIC_output.\")\n",
        "  print(\"This folder contains an initialization file for EXOTIC called inits.json.\")\n",
        "  print(\"The same folder will contain the output files and images when EXOTIC finishes running.\")\n",
        "\n",
        "  if not mobs_data:  \n",
        "    print(f\"\\nThe inits.json file currently says that your observatory latitude was {latitude} deg,\")\n",
        "    print(f\"longitude was {longitude} deg, and elevation was {height}m.  \\n\")\n",
        "    print(\"*** If any of these are incorrect, please change them in the inits.json file. ***\")\n",
        "    print(\"*** (Please make sure that Western longitudes have a negative sign! ***\")\n",
        "    print(\"*** TheSkyX sometimes stamps Western longitudes as positive; this needs to be switched! ***\\n\")\n",
        "  \n",
        "  return(inits_file_path)\n",
        "\n",
        "#########################################################\n",
        "\n",
        "# p is the name of the folder entered by the user.  Decide what to do based on what\n",
        "# is found in the folder.\n",
        "\n",
        "bokeh.io.output_notebook()\n",
        "sample_data = False\n",
        "\n",
        "p = input(\"Press enter to run the Hat-p-32b sample data, or enter a path as described above.\")\n",
        "if p == \"\":\n",
        "  sample_data = True\n",
        "  if os.path.isdir(\"/content/drive/My Drive/sample-data/HatP32Dec202017\"):\n",
        "    print(\"Actually, it looks like you already have sample data, no need to download it again.\")\n",
        "  else:\n",
        "    !git clone https://github.com/rzellem/EXOTIC_sampledata.git /content/drive/My\\ Drive/sample-data\n",
        "  p = \"sample-data/HatP32Dec202017\"\n",
        "\n",
        "p = check_dir(os.path.join(\"/content/drive/My Drive/\", p))\n",
        "output_dir = os.path.join(p, \"EXOTIC_output/\")      \n",
        "                                         \n",
        "inits = []    # array of paths to any inits files found in the directory\n",
        "files = [f for f in sorted(os.listdir(p)) if os.path.isfile(os.path.join(p, f))]\n",
        "fits_count, first_image = 0, \"\"\n",
        "for f in files:\n",
        "  if re.search(r\"\\.f[itz]+s?$\", f, re.IGNORECASE):\n",
        "    if first_image == \"\":\n",
        "      first_image = os.path.join(p, f)\n",
        "    fits_count += 1\n",
        "  if re.search(r\"\\.json$\", f, re.IGNORECASE):\n",
        "    inits.append(os.path.join(p, f))\n",
        "print(f\"Found {fits_count} image files and {len(inits)} initialization files in the directory.\")\n",
        "\n",
        "if fits_count >= 19:                  # more than 20 images in the folder --> run EXOTIC on these images\n",
        "  if len(inits) == 1:                 # one inits file exists\n",
        "    inits_path = os.path.join(p, inits[0])\n",
        "    with open(inits_path) as i_file:\n",
        "      inits_data = i_file.read()\n",
        "      d = json.loads(inits_data)\n",
        "      targ_coords = d[\"user_info\"][\"Target Star X & Y Pixel\"]\n",
        "      comp_coords = d[\"user_info\"][\"Comparison Star(s) X & Y Pixel\"]\n",
        "      input_dir = d[\"user_info\"][\"Directory with FITS files\"]\n",
        "      if input_dir != p:\n",
        "        print(f\"The directory with fits files should be {p} but your inits file says {input_dir}.\")\n",
        "        print(\"This may or may not cause problems.  Just letting you know.\")\n",
        "      print(f\"Coordinates from your inits file:\\ntarget: {targ_coords}\\ncomps: {comp_coords}\")\n",
        "      output_dir = d[\"user_info\"][\"Directory to Save Plots\"]\n",
        "      \n",
        "  else:                               # no inits file: display image and prompt for target, comp coords;\n",
        "                                      # then make an inits file and put it into the output directory (with the plots)\n",
        "    print(\"There are either 0 or > 1 inits files in your image directory, so we'll make a new one.\")\n",
        "    print(\"Displaying first image (this can take a minute).\")\n",
        "    display_image(first_image)\n",
        "    obs = \"\"\n",
        "    print(\"Enter the coordinates of the target in the format [x, y]\")\n",
        "    print(\"Do NOT include decimals in the coordinates.\")\n",
        "    print(\"Be sure to include the square brackets and comma.\")\n",
        "    print(\"Target Coordinates: [x, y]\")\n",
        "    targ_coords = input()  \n",
        "    print(\"\\nEnter the coordinates of your comparison stars in the format [[x1, y1], [x2, y2]]\")\n",
        "    print(\"Be sure to include all square brackets and commas.\")\n",
        "    print(\"Do NOT include decimals in the coordinates.\")    \n",
        "    print(\"(E.g. if you only have one comp, you would enter [[x, y]])\")\n",
        "    print(\"Comparison Star Coordinates: [[x1, y1], [x2, y2]]\")\n",
        "    comp_coords = input()\n",
        "    aavso_obs_code = \"\"\n",
        "    if not sample_data:\n",
        "      print(\"If you have an AAVSO Observer code, enter it here.\")\n",
        "      print(\"If not (or if you are not sure), just press enter.\")\n",
        "      aavso_obs_code = input()\n",
        "#    print(\"If you want an alternate output directory, type it here.\")\n",
        "#    alt_out = input()\n",
        "#    if (re.search(r\"\\w\", alt_out)):\n",
        "#      output_dir = alt_out\n",
        "    if not os.path.isdir(output_dir):\n",
        "      os.mkdir(output_dir)\n",
        "    inits = [make_inits_file(planetary_params, p, output_dir, first_image, targ_coords, comp_coords, obs, aavso_obs_code, sample_data)]\n",
        "\n",
        "  if not os.path.isdir(output_dir):    # Make the output directory if it does not exist already.\n",
        "    os.mkdir(output_dir)\n",
        "  output_dir_for_shell = output_dir.replace(\" \", \"\\ \")\n",
        "\n",
        "# At this point, we have made an inits file if needed and are ready to run it.\n",
        "# Or, if there were < 20 images in the directory (meaning that the directory was \n",
        "# only contained inits files, we'll run those one-by-one.\n",
        "\n",
        "print(\"Path to the inits file(s) that will be used:\")\n",
        "\n",
        "for inits_file in inits:\n",
        "  print(inits_file)\n",
        "\n",
        "num_inits = len(inits)\n",
        "\n",
        "commands = []\n",
        "for inits_file in inits:\n",
        "  with open(inits_file) as i_file:\n",
        "    inits_data = i_file.read()\n",
        "    d = json.loads(inits_data)\n",
        "    date_obs = d[\"user_info\"][\"Observation date\"]\n",
        "    planet = d[\"planetary_parameters\"][\"Planet Name\"]\n",
        "    output_dir = d[\"user_info\"][\"Directory to Save Plots\"]\n",
        "    if not os.path.isdir(output_dir):\n",
        "      os.mkdir(output_dir)\n",
        "    inits_file_for_shell = inits_file.replace(\" \", \"\\\\ \")\n",
        "    run_exotic = str(f\"exotic -red {inits_file_for_shell} -ov\")\n",
        "    debug_exotic_run = str(f\"!exotic -red \\\"{inits_file}\\\" -ov\")\n",
        "\n",
        "    commands.append({\"inits_file_for_shell\": inits_file_for_shell, \"output_dir\": output_dir, \n",
        "                      \"planet\": planet, \"date_obs\": date_obs, \n",
        "                      \"run_exotic\": run_exotic, \"debug_exotic_run\": debug_exotic_run\n",
        "                      })\n",
        "    print(f\"{debug_exotic_run}\")\n",
        "    !eval \"$run_exotic\"\n",
        "\n",
        "    lightcurve = os.path.join(output_dir,\"FinalLightCurve_\"+planet+\"_\"+date_obs+\".png\")\n",
        "    fov = os.path.join(output_dir,\"temp/FOV_\"+planet+\"_\"+date_obs+\"_LinearStretch.png\")\n",
        "    triangle = os.path.join(output_dir,\"temp/Triangle_\"+planet+\"_\"+date_obs+\".png\")\n",
        "    print(f\"lightcurve: {lightcurve}\\nfov: {fov}\\ntriangle: {triangle}\\n\")\n",
        "\n",
        "    if not (os.path.isfile(lightcurve) and os.path.isfile(fov) and os.path.isfile(triangle)):\n",
        "      print(f\"Something went wrong with {planet} {date_obs}.\\nCopy the command below into a new cell and run to find the error:\\n{debug_exotic_run}\\n\")\n",
        "      continue\n",
        "\n",
        "    imageA = widgets.Image(value=open(lightcurve, 'rb').read())\n",
        "    imageB = widgets.Image(value=open(fov, 'rb').read())\n",
        "    hbox = HBox([imageB, imageA])\n",
        "    display(hbox)\n",
        "    display(Image(filename=triangle))\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ma1NbA8bfOUV"
      },
      "source": [
        "**The cell above will take between 15 minutes and an hour to run after you enter your path; please be patient.**  \n",
        "\n",
        "If you see an error message and / or the Colaboratory logo at the top turns red, please post to #data-reduction on the Exoplanet Watch Slack channel.  \n",
        "\n",
        "**Explanation of the plots that are displayed at the end of the run:**\n",
        "\n",
        "The plot titled \"FOV\" is the \"Field of View\" of the images.  Your target is circled, and if EXOTIC used a comparison star, that is circled too.  (EXOTIC does a quick preliminary fit using each of your comps, and chooses the one that gives the cleanest lightcurve to use in the more time-consuming, more rigorous fitting routine.  Sometimes, though, the cleanest lightcurve results from not using *any* comps.  In that case, you will only see your target circled.)\n",
        "\n",
        "The lightcurve shows the dip in light as the planet blocked the star during your observations.  The last plot shows what are called the \"posteriors\" of the fit, obtained by an algorithm that is called \"nested sampling\".  This shows how certain each of the parameters are.  The main parameter of interest is the transit midpoint, tmid.  You should see Gaussian distributions; if you don't, then it might be because the data were noisy.  \n",
        "\n",
        "All of the graphs above, plus several other output files, are now saved to your output directory (if you used this Colaboratory to make the inits file, that is in the EXOTIC_output folder within the folder of images on your Google drive).  The textfile whose name starts with \"AAVSO\" is already in the correct format to be uploaded to the American Association of Variable Star Observers' [Exoplanet Database](https://www.aavso.org/apps/exosite/doc).  Uploading will allow other astronomers to use your data to plan their observing campaigns for your target, and will allow NASA to use your data to optimize space telescopes.  (However, please do not upload the sample data to AAVSO; that has already been done.)\n",
        "\n",
        "\n",
        "**Note that you can use this same Colaboratory again to reduce your next transit!**\n",
        "\n",
        "### <font color='red'>Cells below this line might be relevant if you have multiple reduced transits of a single target; otherwise, just ignore them.</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbzqg9UYVOEc",
        "cellView": "form"
      },
      "source": [
        "#@title Display output from multiple runs.\n",
        "# Displays the output for folders within the path that is passed in.\n",
        "\n",
        "#COMP_STAR-XC=[{\"ra\": \"303.22043818809965\", \"dec\": \"65.24542149217169\", \"x\": \"463\", \"y\": \"176\"}]\n",
        "\n",
        "########################################################\n",
        "\n",
        "def get_comp(aavso_filepath):\n",
        "  print(f\"In get_comp: aavso_filepath {aavso_filepath}\")\n",
        "  found_comp = False\n",
        "  with open(aavso_filepath, 'r') as aavso:\n",
        "    for line in aavso:\n",
        "      m = re.search(\"#COMP_STAR-XC=\\[\\]\", line)   # Return empty strings if no comp star was used.\n",
        "      if m:\n",
        "        return(\"no comp,no comp,no comp,no comp\")\n",
        "      m = re.search(\"#COMP_STAR-XC=\\[\\{\\\"ra\\\"\\: \\\"(\\d+\\.\\d+)\\\", \\\"dec\\\"\\: \\\"([+-]?\\d+\\.\\d+)\\\", \\\"x\\\"\\: \\\"(\\d+)\\\", \\\"y\\\"\\: \\\"(\\d+)\\\"\\}\\]\", line)\n",
        "      if m:\n",
        "        comp_ra, comp_dec, comp_x, comp_y = m.group(1), m.group(2), m.group(3), m.group(4)\n",
        "        print (f\"Got comp_ra {comp_ra}, comp_dec {comp_dec}, comp_x {comp_x}, comp_y {comp_y}\")\n",
        "        return(f\"{comp_ra},{comp_dec},{comp_x},{comp_y}\")\n",
        "      m = re.search(\"#COMP_STAR-XC=\\[\\{\\\"ra\\\"\\: null, \\\"dec\\\"\\: null, \\\"x\\\"\\: \\\"(\\d+)\\\", \\\"y\\\"\\: \\\"(\\d+)\\\"\\}\\]\", line)\n",
        "      if m:\n",
        "        comp_ra, comp_dec, comp_x, comp_y = \"null\", \"null\", m.group(1), m.group(2)\n",
        "        return(f\"{comp_ra},{comp_dec},{comp_x},{comp_y}\")\n",
        "      \n",
        "\n",
        "########################################################\n",
        "\n",
        "def make_fpp_header(fpp):\n",
        "  header = \"\"\n",
        "  header_fields = sorted(fpp.keys())\n",
        "  for field in header_fields:\n",
        "    header = header + field + \",\"\n",
        "  header = header +\"comp RA,comp Dec,comp x,comp y\"\n",
        "  return(header+\"\\n\")\n",
        "\n",
        "########################################################\n",
        "\n",
        "def include_params(fpp, aavso_filepath):\n",
        "  params = \"\"\n",
        "  fields = sorted(fpp.keys())\n",
        "  for field in fields:\n",
        "    params = params + fpp[field] + \",\"\n",
        "  params = params + get_comp(aavso_filepath)\n",
        "  return(params+\"\\n\")\n",
        "\n",
        "########################################################\n",
        "\n",
        "def get_midt_err(fpp):\n",
        "  midt = 0.0\n",
        "  if \"Mid-Transit Time\" in fpp.keys():\n",
        "    midt = fpp[\"Mid-Transit Time\"]\n",
        "  elif \"Mid-Transit Time (Tmid)\" in fpp.keys():\n",
        "    midt = fpp['Mid-Transit Time (Tmid)']\n",
        "  m = re.search(r\"^\\d+\\.\\d+\\s\\+\\/\\-\\s(\\d+\\.\\d+)\", midt)\n",
        "  if m:\n",
        "    midt_err = float(m.group(1))\n",
        "    return(midt_err)\n",
        "  else:\n",
        "    print(f\"PROBLEM!  Cannot find midtransit time error!!!\\n  {fpp}\")\n",
        "\n",
        "########################################################\n",
        "\n",
        "def display_output(p, minimum_residual_scatter, min_midt_err, p_param_dict):\n",
        "  fpps = \"\"\n",
        "  dirs = [d for d in sorted(os.listdir(p)) if os.path.isdir(os.path.join(p, d))]\n",
        "  for d in dirs:\n",
        "    print(f\"Searching directory {d}\")\n",
        "    output_images, fov_descs, date, lc_path = [], [], \"\", \"\"\n",
        "    files = [f for f in sorted(os.listdir(os.path.join(p, d))) if os.path.isfile(os.path.join(p, d, f))]\n",
        "    for f in files:\n",
        "      if f == \"wcs.fits\" or re.search(r\"Observing_Statistics\", f):\n",
        "        continue\n",
        "      m = re.search(r\"([^_]+)_([^_]+)_([^_^\\.]+)_?([^\\.]*)\\.(.+)\", f)\n",
        "      if m:\n",
        "        desc, target, date, desc2, filetype = m.group(1), m.group(2), m.group(3), m.group(4), m.group(5)\n",
        "        if (target != p_param_dict[\"Planet Name\"]):\n",
        "          continue\n",
        "        fp = os.path.join(p,d,f)\n",
        "        if desc == \"FinalLightCurve\" and filetype == \"png\":\n",
        "          image = widgets.Image(value=open(fp, 'rb').read())\n",
        "          output_images.append(image)\n",
        "        if desc == \"FinalParams\":\n",
        "          with open(fp) as o_file:\n",
        "            final_params = o_file.read()\n",
        "            params = json.loads(final_params)\n",
        "            fpp = params[\"FINAL PLANETARY PARAMETERS\"]\n",
        "            if \"Best Comparison Star\" in fpp:\n",
        "              del fpp[\"Best Comparison Star\"]\n",
        "              del fpp[\"Optimal Annulus\"]\n",
        "              del fpp[\"Optimal Aperture\"]\n",
        "            if fpps == \"\":\n",
        "              fpps = \"folder,\" + make_fpp_header(fpp)\n",
        "            pprint(fpp)\n",
        "            scatter = fpp['Scatter in the residuals of the lightcurve fit is']\n",
        "            m = re.search(r\"(\\d+\\.\\d+)\", scatter)\n",
        "            scatter = float(m.group(1))\n",
        "            if scatter < minimum_residual_scatter and get_midt_err(fpp) < min_midt_err:\n",
        "              fpps = fpps + os.path.join(p,d) + \",\" + include_params(fpp, os.path.join(p, d, \"AAVSO_\"+target+\"_\"+date+\".txt\"))\n",
        "        if desc == \"FOV\" and filetype == \"png\":\n",
        "          if (desc2 == \"SquaredStretch\"):\n",
        "            image = widgets.Image(value=open(fp, 'rb').read())\n",
        "            output_images.append(image)\n",
        "            fov_descs.append(desc2)\n",
        "\n",
        "    lc_first = output_images[::-1] \n",
        "    if (len(output_images)>0):\n",
        "      print(f\"{d} {date}, {fov_descs[::-1]}\")\n",
        "      hbox = HBox(lc_first)\n",
        "      display(hbox)\n",
        "      print(\"\\n*********************************************************\\n\")\n",
        "\n",
        "    fpps = fpps + display_output(os.path.join(p, d), minimum_residual_scatter, min_midt_err, p_param_dict)\n",
        "\n",
        "  return(fpps)\n",
        "\n",
        "########################################################\n",
        "\n",
        "def reformat_header(header):\n",
        "  header_list = header.split(',')\n",
        "  new_header_list = []\n",
        "  midt_field_index = -1\n",
        "  for field in header_list:\n",
        "    midt_field_index += 1\n",
        "    if field == \"folder\" or field == \"comp RA\" or field == \"comp Dec\" or field == \"comp x\" or field == \"comp y\" or re.search(r'^Scatter', field) or re.search(r'Transit depth uncertainty', field):\n",
        "      new_header_list.append(field)\n",
        "      continue\n",
        "    else:\n",
        "      new_header_list.append(field)\n",
        "      new_header_list.append(field+\" ERROR\")  \n",
        "  new_header = \",\".join(new_header_list)\n",
        "  return(new_header, new_header_list)\n",
        "\n",
        "##############################################\n",
        "\n",
        "def reformat_fpp_list(fpps, t0, period):\n",
        "  fpp_list = []\n",
        "  orig_fpp_list = fpps.split('\\n')\n",
        "  orig_header = orig_fpp_list[0]\n",
        "  for fpp in orig_fpp_list:\n",
        "    if (fpp == orig_header):\n",
        "      continue\n",
        "    fpp_list.append(fpp)\n",
        "  \n",
        "  header, field_list = reformat_header(orig_header)\n",
        "  \n",
        "  for i in range(len(fpp_list)):\n",
        "    fpp_list[i] = fpp_list[i].replace(\" +/- \", \",\")\n",
        "    fp_list = fpp_list[i].split(',')\n",
        "    for j in range(len(fp_list)):\n",
        "      if j == 0:\n",
        "        continue\n",
        "      m = re.search(r'^(\\d+\\.?\\d*)', fp_list[j])\n",
        "      if m:\n",
        "        fp_list[j] = m.group(1)\n",
        "    fpp_list[i] = \",\".join(fp_list)\n",
        "\n",
        "  fpp_dicts = {}\n",
        "  for i in range(len(fpp_list)):\n",
        "    transit_fpp = fpp_list[i].split(',')\n",
        "    folder = transit_fpp[0]\n",
        "    if not re.search(r'\\w', folder):\n",
        "      continue\n",
        "    fpp_dict = { \"all_vals\": transit_fpp }\n",
        "\n",
        "    for j in range(len(field_list)):\n",
        "      field = field_list[j]\n",
        "      value = transit_fpp[j]\n",
        "      fpp_dict[field] = value\n",
        "    epoch = round((float(fpp_dict[\"Mid-Transit Time (Tmid)\"]) - t0)/period)\n",
        "    while epoch in fpp_dicts.keys():    #  Just in case the same transit is observed by 2 different telescopes.\n",
        "      epoch += 0.00001\n",
        "    fpp_dicts[epoch] = fpp_dict\n",
        "\n",
        "  obs_midt_list, obs_midt_unc_list = [], []\n",
        "\n",
        "  sorted_fpp_list = []\n",
        "  for key in sorted(fpp_dicts.keys()):\n",
        "    all_vals = fpp_dicts[key][\"all_vals\"]\n",
        "    sorted_fpp_list.append(all_vals)\n",
        "    obs_midt, obs_midt_unc = 0.0, 0.0\n",
        "    if \"Mid-Transit Time\" in fpp_dicts[key].keys():\n",
        "      obs_midt = fpp_dicts[key][\"Mid-Transit Time\"]\n",
        "      obs_midt_unc = fpp_dicts[key][\"Mid-Transit Time ERROR\"]\n",
        "    elif \"Mid-Transit Time (Tmid)\" in fpp_dicts[key].keys():\n",
        "      obs_midt = fpp_dicts[key]['Mid-Transit Time (Tmid)']\n",
        "      obs_midt_unc = fpp_dicts[key][\"Mid-Transit Time (Tmid) ERROR\"]\n",
        "    obs_midt_list.append(obs_midt)\n",
        "    obs_midt_unc_list.append(obs_midt_unc)\n",
        "\n",
        "  return(header, sorted_fpp_list, fpp_dicts, obs_midt_list, obs_midt_unc_list)\n",
        "\n",
        "##############################################\n",
        "\n",
        "def observed_minus_calculated(midt, midt_unc, t0, t0_unc, period, period_unc):\n",
        "  epoch = round((midt - t0)/period)\n",
        "\n",
        "  calc = t0 + epoch*period    # Equation 1 from Zellem et. al., 2020.\n",
        "  obs_minus_calc = midt - calc\n",
        "\n",
        "  calc_unc = ((epoch**2) * (period_unc**2))\n",
        "  calc_unc += (2 * epoch * period_unc * t0_unc)\n",
        "  calc_unc += (t0_unc**2)\n",
        "  calc_unc = math.sqrt(calc_unc)       # Equation 3 from Zellem et. al., 2020.\n",
        "\n",
        "  obs_minus_calc_unc = math.sqrt(midt_unc**2 + calc_unc**2)   # sum the uncertainties of observed and calculated values in quadrature\n",
        "  \n",
        "  # Convert from days to minutes:\n",
        "  obs_minus_calc = obs_minus_calc * 24 * 60\n",
        "  obs_minus_calc_unc = obs_minus_calc_unc * 24 * 60\n",
        "\n",
        "  return(epoch, obs_minus_calc, obs_minus_calc_unc)\n",
        "\n",
        "#######################################################\n",
        "\n",
        "def find_o_c(t0, t0_unc, period, period_unc, obs_midt_list, obs_midt_unc_list):\n",
        "  epochs, o_c, o_c_unc = [], [], []\n",
        "  for i in range(len(obs_midt_list)):\n",
        "    midt, midt_unc = float(obs_midt_list[i]), float(obs_midt_unc_list[i])\n",
        "    epoch, obs_minus_calc, obs_minus_calc_unc = observed_minus_calculated(midt, midt_unc, t0, t0_unc, period, period_unc)\n",
        "    epochs.append(epoch)\n",
        "    o_c.append(obs_minus_calc)\n",
        "    o_c_unc.append(obs_minus_calc_unc)\n",
        "\n",
        "  return(epochs, o_c, o_c_unc)\n",
        "\n",
        "#######################################################\n",
        "\n",
        "def get_plot_xrange (e):\n",
        "  max_x = max(e)\n",
        "  if max(e) > max_x:\n",
        "    max_x = max(e)\n",
        "\n",
        "  min_x = min(e)\n",
        "  if min(e) < min_x:\n",
        "    min_x = min(e)\n",
        "\n",
        "  x_overflow = (max_x - min_x)/15\n",
        "  min_x -= x_overflow\n",
        "  max_x += x_overflow\n",
        "\n",
        "  return(max_x, min_x, x_overflow)\n",
        "\n",
        "###############################################################\n",
        "\n",
        "def make_plot (planet, epoch, o_c, uncertainty):\n",
        "\n",
        "  plot = bk.plotting.figure(\n",
        "    plot_width=600, plot_height=400,\n",
        "    title = \"%s O - C\" % (planet),\n",
        "    x_axis_label = \"Epoch (cycle)\", y_axis_label = \"O - C (min)\", \n",
        "  )\n",
        "  \n",
        "  err_xs = []\n",
        "  err_ys = []\n",
        "\n",
        "  for x, y, yerr in zip(epoch, o_c, uncertainty):\n",
        "    err_xs.append((x, x))\n",
        "    err_ys.append((y - yerr, y + yerr))\n",
        "\n",
        "  max_x, min_x, x_overflow = get_plot_xrange (epoch)\n",
        "\n",
        "  x_axis = [min_x, max_x]\n",
        "  zeros = [0, 0]\n",
        "\n",
        "  plot.x_range = Range1d(min_x, max_x)\n",
        "  \n",
        "  plot.multi_line(err_xs, err_ys, color='grey') \n",
        "  plot.scatter(epoch, o_c, size=6, color='blue')\n",
        "\n",
        "  plot.line(x_axis, zeros, color='black')\n",
        "\n",
        "  m,b = np.polyfit(epoch,o_c,1)\n",
        "  x_vals = [min_x,max_x]\n",
        "  y_vals = [b+m*min_x,b+m*max_x]\n",
        "  y_avg = (y_vals[1]+y_vals[0])/2.0\n",
        "  # remove the # from the next lines to add line of best fit, slope, and average y value across this plot\n",
        "  # plot.line(x_vals,y_vals,color=\"blue\")\n",
        "  # print(\"slope: \",m,\"\\naverage y value: \",y_avg)\n",
        "\n",
        "  plot.title.text_font_size = '14pt'\n",
        "  plot.xaxis.axis_label_text_font_size = \"14pt\"\n",
        "  plot.yaxis.axis_label_text_font_size = \"14pt\"\n",
        "  return(plot)\n",
        "\n",
        "##############################################\n",
        "\n",
        "def obs_calc_plot (fpps, t0, t0_unc, period, period_unc):\n",
        "  header, fpp_list, fpp_dicts, obs_midt_list, obs_midt_unc_list = reformat_fpp_list(fpps, t0, period)  # fpp_list now sorted by epoch\n",
        "\n",
        "  epochs, o_c, o_c_unc = find_o_c(t0, t0_unc, period, period_unc, obs_midt_list, obs_midt_unc_list)\n",
        "\n",
        "  bk.io.output_notebook()\n",
        "  plot = make_plot(planet, epochs, o_c, o_c_unc)\n",
        "  bk.plotting.show(plot)\n",
        "\n",
        "  out_str = str(header+\",epoch,o_c (min),o_c_unc (min),midt (BJD),midt_unc (BJD)\\n\")\n",
        "  for i in range(len(fpp_list)):\n",
        "    fp, epoch, o_c_val, o_c_unc_val, obs_midt_val, obs_midt_unc_val = fpp_list[i], epochs[i], o_c[i], o_c_unc[i], obs_midt_list[i], obs_midt_unc_list[i]\n",
        "    all_transit_data = str(f\"{','.join(fp)},{epoch},{o_c_val},{o_c_unc_val},{obs_midt_val},{obs_midt_unc_val}\")\n",
        "    out_str = out_str + all_transit_data + \"\\n\"\n",
        "  print(out_str)\n",
        "\n",
        "  return(epochs, obs_midt_list, obs_midt_unc_list, fpp_list)\n",
        "\n",
        "##############################################\n",
        "\n",
        "# Kalee's folders:\n",
        "# WASP-35b\n",
        "# The Orbiteers/EXOTIC\n",
        "# Wasp-43b/output\n",
        "# 2021 Qatar-1b inits/EXOTIC_output\n",
        "# 2021 Tres-1b inits/EXOTIC_output\n",
        "\n",
        "import math\n",
        "import bokeh as bk\n",
        "import bokeh.io\n",
        "import bokeh.plotting\n",
        "from bokeh.models import Range1d, Label\n",
        "\n",
        "planet = p_param_dict[\"Planet Name\"]\n",
        "t0 = float(p_param_dict[\"Published Mid-Transit Time (BJD-UTC)\"])\n",
        "t0_unc = float(p_param_dict[\"Mid-Transit Time Uncertainty\"])\n",
        "period = float(p_param_dict[\"Orbital Period (days)\"])\n",
        "period_unc = float(p_param_dict[\"Orbital Period Uncertainty\"])\n",
        "\n",
        "minimum_residual_scatter = 2.2   # This is the minimum scatter in the residuals of the lightcurve fit\n",
        "                                 # that you will accept as a \"decent\" lightcurve.\n",
        "\n",
        "min_midt_err = 0.007             # Similarly, this is the minimum midtransit time uncertainty that \n",
        "                                 # that you will accept as a \"decent\" lightcurve\n",
        "\n",
        "print(\"This cell recursively searches the folder you input for EXOTIC output files\\ncorresponding to the target you looked up in the second cell.\\n\")\n",
        "print(f\"It displays all lightcurves, FOV's, and final planetary parameters for that target.\\n*If* the residual scatter is less than {minimum_residual_scatter}% and the midtransit time uncertainty\\nis less than {min_midt_err}, it includes the final planetary parameters in the comma-\\ndelimited list at the end, and includes the corresponding point on the O-C plot.\\n\")\n",
        "\n",
        "p = input(\"Give the path to a directory that contains folders of EXOTIC output:\")\n",
        "if not re.search(r\"^\\/content\\/drive\\/My Drive\", p):\n",
        "  p = os.path.join(\"/content/drive/My Drive\", p)\n",
        "if not p[len(p)-1] == '/':\n",
        "  p = p+\"/\"\n",
        "print(p)\n",
        "\n",
        "fpps = display_output(p, minimum_residual_scatter, min_midt_err, p_param_dict)\n",
        "epochs, obs_midt_list, obs_midt_unc_list, fpp_list = obs_calc_plot(fpps, t0, t0_unc, period, period_unc)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lV6BniLGdyT0",
        "cellView": "form"
      },
      "source": [
        "#@title Ephemeris Fitter (requires target lookup & display output cells above)\n",
        "\n",
        "# Kyle Pearson's Ephemeris Fitter (installation and helper functions)\n",
        "'''\n",
        "Nested sampling for linear fitting\n",
        "\n",
        "K. PEARSON\n",
        "'''\n",
        "\n",
        "##############################################\n",
        "\n",
        "class linear_fitter(object):\n",
        "    def __init__(self, time, data, dataerr, bounds):\n",
        "        self.time = time\n",
        "        self.data = data\n",
        "        self.dataerr = dataerr\n",
        "        self.bounds = bounds\n",
        "        self.fit_nested()\n",
        "\n",
        "    def fit_nested(self):\n",
        "        freekeys = list(self.bounds.keys())\n",
        "        boundarray = np.array([self.bounds[k] for k in freekeys])\n",
        "        bounddiff = np.diff(boundarray,1).reshape(-1)\n",
        "\n",
        "        def loglike(pars):\n",
        "            # chi-squared\n",
        "            model = pars[0]*self.time + pars[1]\n",
        "            return -0.5 * np.sum( ((self.data-model)/self.dataerr)**2 )\n",
        "        \n",
        "        def prior_transform(upars):\n",
        "            # transform unit cube to prior volume\n",
        "            return (boundarray[:,0] + bounddiff*upars)\n",
        "        \n",
        "        dsampler = dynesty.DynamicNestedSampler(\n",
        "            loglike, prior_transform,\n",
        "            ndim=len(freekeys), bound='multi', sample='unif', \n",
        "            maxiter_init=5000, dlogz_init=1, dlogz=0.05,\n",
        "            maxiter_batch=100, maxbatch=10, nlive_batch=100\n",
        "        )\n",
        "        dsampler.run_nested()\n",
        "        self.results = dsampler.results\n",
        "\n",
        "        # alloc data for best fit + error\n",
        "        self.errors = {}\n",
        "        self.quantiles = {}\n",
        "        self.parameters = {}\n",
        "\n",
        "        tests = [{} for i in range(4)]\n",
        "\n",
        "        # Derive kernel density estimate for best fit\n",
        "        weights = np.exp(self.results.logwt - self.results.logz[-1])\n",
        "        samples = self.results['samples']\n",
        "        logvol = self.results['logvol']\n",
        "        wt_kde = gaussian_kde(resample_equal(-logvol, weights))  # KDE\n",
        "        logvol_grid = np.linspace(logvol[0], logvol[-1], 1000)  # resample\n",
        "        wt_grid = wt_kde.pdf(-logvol_grid)  # evaluate KDE PDF\n",
        "        self.weights = np.interp(-logvol, -logvol_grid, wt_grid)  # interpolate\n",
        "\n",
        "        # errors + final values\n",
        "        mean, cov = dynesty.utils.mean_and_cov(self.results.samples, weights)\n",
        "        mean2, cov2 = dynesty.utils.mean_and_cov(self.results.samples, self.weights)\n",
        "        for i in range(len(freekeys)):\n",
        "            self.errors[freekeys[i]] = cov[i,i]**0.5\n",
        "            tests[0][freekeys[i]] = mean[i]\n",
        "            tests[1][freekeys[i]] = mean2[i]\n",
        "\n",
        "            counts, bins = np.histogram(samples[:,i], bins=100, weights=weights)\n",
        "            mi = np.argmax(counts)\n",
        "            tests[3][freekeys[i]] = bins[mi] + 0.5*np.mean(np.diff(bins))\n",
        "\n",
        "            # finds median and +- 2sigma, will vary from mode if non-gaussian\n",
        "            self.quantiles[freekeys[i]] = dynesty.utils.quantile(self.results.samples[:,i], [0.025, 0.5, 0.975], weights=weights)\n",
        "            tests[2][freekeys[i]] = self.quantiles[freekeys[i]][1]\n",
        "\n",
        "        # find best fit\n",
        "        chis = []\n",
        "        res = []\n",
        "        for i in range(len(tests)):\n",
        "            model = self.time*tests[i]['m'] + tests[i]['b']\n",
        "            residuals = self.data - model\n",
        "            chis.append(np.sum(residuals**2))\n",
        "\n",
        "        mi = np.argmin(chis)\n",
        "        self.parameters = copy.deepcopy(tests[mi])\n",
        "\n",
        "        # final model\n",
        "        self.model = self.time *self.parameters['m'] + self.parameters['b']\n",
        "        self.residuals = self.data - self.model\n",
        "\n",
        "    def plot_triangle(self,savefile=None):\n",
        "        \n",
        "        # triangle plot \n",
        "        fig,axs = dynesty.plotting.cornerplot(self.results, labels=list(self.bounds.keys()), quantiles_2d=[0.4,0.85], smooth=0.015, show_titles=True,use_math_text=True, title_fmt='.2e',hist2d_kwargs={'alpha':1,'zorder':2,'fill_contours':False})\n",
        "        dynesty.plotting.cornerpoints(self.results, labels=list(self.bounds.keys()), fig=[fig,axs[1:,:-1]],plot_kwargs={'alpha':0.1,'zorder':1,} )\n",
        "        plt.tight_layout()\n",
        "\n",
        "        if savefile:\n",
        "            plt.savefig(savefile)\n",
        "        else:\n",
        "            plt.show()\n",
        "\n",
        "\n",
        "    def plot_oc(self,savefile=None):\n",
        "        # O-C plot\n",
        "        f,ax = plt.subplots(2)\n",
        "        ax[0].errorbar(self.time, self.data, yerr=self.dataerr, ls='none', marker='o',label='Data')\n",
        "        ax[0].plot(self.time, self.model, 'b--', label='Model')\n",
        "        ax[0].set_ylabel(\"Julian Day\")\n",
        "        ax[0].set_title(\"Linear Ephemeris Fit\")\n",
        "        ax[1].errorbar(self.time, self.residuals*24*60, yerr=self.dataerr*24*60, ls='none', marker='o',label='Data')\n",
        "        ax[1].axhline(0,ls='--',label=\"P: {:.5f}+-{:.5f}, Tmid: {:.5f}+-{:.5f}\".format(self.parameters['m'], self.errors['m'], self.parameters['b'], self.errors['b']))\n",
        "        ax[1].legend(loc='best')\n",
        "        ax[1].set_xlabel(\"Epoch\")\n",
        "        ax[1].set_ylabel(\"Residuals [min]\")\n",
        "        plt.show()\n",
        "\n",
        "##############################################\n",
        "\n",
        "def do_ephemeris_fit(epochs, obs_midt_list, obs_midt_unc_list, start_index, period, period_unc):\n",
        "  t0_lower_bound = float(obs_midt_list[start_index]) - float(obs_midt_unc_list[start_index])\n",
        "  t0_upper_bound = float(obs_midt_list[start_index]) + float(obs_midt_unc_list[start_index])\n",
        "\n",
        "  b = float(obs_midt_list[start_index])\n",
        "  raw_epochs = np.array(epochs)\n",
        "  times = np.array([ float(t) for t in obs_midt_list[start_index:len(obs_midt_list)]])\n",
        "  error = np.array([ float(t) for t in obs_midt_unc_list[start_index:len(obs_midt_unc_list)]])\n",
        "\n",
        "  p_unc = 0.0007\n",
        "  if period_unc > 0.0007:\n",
        "    p_unc = period_unc      # Ensure that the period uncertainty is at least a minute.\n",
        "\n",
        "  bounds = {'m':[(period-p_unc), (period+p_unc)], 'b':[t0_lower_bound-.005, t0_upper_bound+.005] }\n",
        "\n",
        "  new_epochs = []\n",
        "  for epoch in sorted(raw_epochs):\n",
        "    new_epochs.append(epoch - epochs[start_index])\n",
        "  epochs_from_first_obs = np.array(new_epochs)\n",
        "\n",
        "  lf = linear_fitter( epochs_from_first_obs, times, error, bounds )\n",
        "  print(lf.parameters['m'], '+-', lf.errors['m'])\n",
        "  print(lf.parameters['b'], '+-', lf.errors['b'])\n",
        "  lf.plot_triangle()\n",
        "  lf.plot_oc()\n",
        "\n",
        "  new_period = lf.parameters['m']\n",
        "  new_period_unc = lf.errors['m']\n",
        "  new_midt = lf.parameters['b']\n",
        "  new_midt_unc = lf.errors['b']\n",
        "\n",
        "  return(new_period, new_period_unc, new_midt, new_midt_unc)\n",
        "\n",
        "########################################################################\n",
        "\n",
        "epochs, obs_midt_list, obs_midt_unc_list, fpp_list = obs_calc_plot(fpps, t0, t0_unc, period, period_unc)\n",
        "\n",
        "start_obs_index = 0\n",
        "new_period, new_period_unc, new_midt, new_midt_unc = do_ephemeris_fit(epochs, obs_midt_list, obs_midt_unc_list, start_obs_index, period, period_unc)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNWhoizvHw46",
        "cellView": "form"
      },
      "source": [
        "#@title Cone Plot (run after fitting a new ephemeris)\n",
        "####################################################################\n",
        "\n",
        "def define_cone_extremes(target_date, t0, t0_unc, period, period_unc):\n",
        "  \n",
        "  epoch = round((target_date.jd - t0)/period)   # t0 in BJD, target_date in JD; the (8 min max) difference won't significantly affect this.\n",
        "\n",
        "  calc = t0 + epoch*period             # Equation 1 from Zellem et. al., 2020.\n",
        "  calc_unc = ((epoch**2) * (period_unc**2))\n",
        "  calc_unc += (2 * epoch * period_unc * t0_unc)\n",
        "  calc_unc += (t0_unc**2)\n",
        "  calc_unc = math.sqrt(calc_unc)       # Equation 3 from Zellem et. al., 2020.\n",
        "\n",
        "  calc = round(calc, 5)\n",
        "  calc_unc = round(calc_unc, 6)\n",
        "\n",
        "  return (epoch, calc, calc_unc)\n",
        "\n",
        "####################################################################\n",
        "\n",
        "def initialize_cone_graph (xs, ys, colors, graph_title, graph_subtitle):\n",
        "\n",
        "  graph = figure(x_axis_label = \"Epoch (cycle) based on NEA ephemeris\", y_axis_label = \"Prediction Uncertainty\", plot_width=800, plot_height=300)\n",
        "  graph.add_layout(Title(text=graph_subtitle, text_font_style=\"italic\", text_font_size=\"12pt\"), 'above')\n",
        "  graph.add_layout(Title(text=graph_title, text_font_size=\"14pt\"), 'above')\n",
        "\n",
        "  graph.xaxis.axis_label_text_font_size = \"14pt\"\n",
        "  graph.yaxis.axis_label_text_font_size = \"14pt\"\n",
        "\n",
        "  for i in range(len(xs)):\n",
        "    x, y, graph_color = xs[i], ys[i], colors[i]\n",
        "    graph.patch(x, y, color=graph_color, fill_alpha=0.3)\n",
        "  return(graph)\n",
        "\n",
        "####################################################################\n",
        "\n",
        "def plot_cone(planet, target_date_string, new_period, new_period_unc, new_midt, \n",
        "              new_midt_unc, t0, t0_unc, period, period_unc, num_transits, epochs):\n",
        "\n",
        "  target_date = Time(target_date_string, format='isot', scale='utc')\n",
        "  old_target_epoch, old_prediction, old_prediction_unc = define_cone_extremes(target_date, t0, t0_unc, period, period_unc)\n",
        "\n",
        "  new_midt_str = str(\"%.5f\" % new_midt)\n",
        "  new_midt_unc_str = (\"%.6f\" % new_midt_unc)\n",
        "  new_period_str = str(\"%.8f\" % new_period)\n",
        "  new_period_unc_str = (\"%.9f\" % new_period_unc)\n",
        "\n",
        "  nea_t0_date = Time(t0, format='jd')\n",
        "  nea_t0_date.format = 'iso'\n",
        "  new_t0_date = Time(new_midt, format='jd')\n",
        "  new_t0_date.format = 'iso'\n",
        "\n",
        "  first_transit_epoch, last_transit_epoch = min(epochs), max(epochs)\n",
        "  last_transit_date = Time((t0 + (period*last_transit_epoch)), format='jd')\n",
        "  last_transit_date.format='iso'\n",
        "  last_transit_yr_month = str(last_transit_date.utc)[0:7]\n",
        "  observation_range = (last_transit_date - new_t0_date)/365.25  # span of observations in years\n",
        "\n",
        "  new_target_epoch, new_prediction, new_prediction_unc = define_cone_extremes(target_date, new_midt, new_midt_unc, new_period, new_period_unc)\n",
        "\n",
        "  new_epoch_zero = old_target_epoch - new_target_epoch\n",
        "  prediction_difference_days = new_prediction - old_prediction\n",
        "  prediction_difference = str(\"%.1f\" % ((prediction_difference_days)*24*60))\n",
        "  pred_unc_decrease = str(\"%.1f\" % (old_prediction_unc / new_prediction_unc))\n",
        "\n",
        "  print(f\"The target date is: {target_date}, and has jd {target_date.jd} (in code at bottom of cell if you want to adjust).\")\n",
        "  print(f\"The NEA ephemeris had midpoint {t0} +/- {t0_unc} BJD ({nea_t0_date.utc}) and period {period} +/- {period_unc} days.\")\n",
        "  print(f\"The new ephemeris has midpoint {new_midt_str} +/- {new_midt_unc_str} BJD ({new_t0_date}) and period {new_period_str} +/- {new_period_unc_str} days.\\n\")\n",
        "  print(f\"Target date {target_date_string[0:10]} is Epoch {old_target_epoch} based on the NEA ephemeris but Epoch {new_target_epoch} based on the new ephemeris.\")\n",
        "  print(f\"(In other words, the new Epoch 0 is Epoch {new_epoch_zero} in the epoch naming convention from the NEA ephemeris.)\\n\")\n",
        "  print(f\"Based on {num_transits} transits, the new midpoint prediction for the target date is {new_prediction} +/- {new_prediction_unc}, which is {prediction_difference}\\nminutes different from the NEA prediction of {old_prediction} +/- {old_prediction_unc}.\\n\")\n",
        "\n",
        "  if (float(pred_unc_decrease) > 1):\n",
        "    print(f\"The uncertainty for {target}\\'s midtransit time on {target_date_string[0:10]} has decreased by a factor of {pred_unc_decrease}!\\n\")\n",
        "  else:\n",
        "    print(f\"These transits alone have not improved upon the NEA ephemeris for {target}\\'s midtransit time on {target_date_string[0:10]}; we need some more!\\n\")\n",
        "\n",
        "  old_x = [0, old_target_epoch, old_target_epoch, 0]\n",
        "  old_y = [t0_unc, old_prediction_unc, -1*old_prediction_unc, -1*t0_unc]\n",
        "\n",
        "  new_x = [new_epoch_zero, new_epoch_zero+new_target_epoch, new_epoch_zero+new_target_epoch, new_epoch_zero]\n",
        "  new_y = [new_midt_unc, new_prediction_unc, -1*new_prediction_unc, -1*new_midt_unc]\n",
        "  new_y_with_offset = [new_midt_unc, prediction_difference_days+new_prediction_unc, prediction_difference_days-new_prediction_unc, -1*new_midt_unc]\n",
        "\n",
        "  title = str(f\"{planet} Midtransit Prediction on Target Date {target_date_string[0:10]}\")\n",
        "\n",
        "  graph = initialize_cone_graph([old_x], [old_y], ['blue'], title, \"Previous Ephemeris\")\n",
        "  show(graph)\n",
        "\n",
        "  graph = initialize_cone_graph([new_x], [new_y], ['red'], title, \"New Ephemeris\")\n",
        "  show(graph)\n",
        "\n",
        "  graph = initialize_cone_graph([old_x, new_x], [old_y, new_y], ['blue', 'red'], title, \"Previous and New Ephemerides Superimposed\")\n",
        "  show(graph)\n",
        "\n",
        "  graph = initialize_cone_graph([old_x, new_x], [old_y, new_y_with_offset], ['blue', 'red'], title, \"Previous and New Ephemerides Superimposed With Prediction Difference Offset\")\n",
        "  show(graph)\n",
        "\n",
        "  out_table = str(f\"NEA midt,NEA midt yr,NEA midt unc,NEA per,NEA per unc,NEA midt_unc on {target_date},Obs start(month, year),Obs end(month, year),Obs range(yrs),Num midpts,New midt unc on {target_date}\\n\")\n",
        "  out_table += str(f\"{t0},{str(nea_t0_date.utc)[0:7]},{t0_unc},{period},{period_unc},{old_prediction_unc},{str(new_t0_date)[0:7]},{last_transit_yr_month},{observation_range},{num_transits},{prediction_difference},{new_prediction_unc}\")\n",
        "  print(out_table)\n",
        "\n",
        "####################################################################\n",
        "from bokeh.models import Title\n",
        "\n",
        "target_date_string = \"2022-01-01T00:00:00\"\n",
        "\n",
        "num_transits = len(fpp_list)\n",
        "\n",
        "plot_cone(planet, target_date_string, new_period, new_period_unc, new_midt, \n",
        "          new_midt_unc, t0, t0_unc, period, period_unc, num_transits, \n",
        "          epochs)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}